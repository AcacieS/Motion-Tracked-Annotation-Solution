{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NtufXocEA0t9",
        "outputId": "1bf8a233-19bf-415c-ed1d-fc498524dbee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: nvidia-smi: command not found\n",
            "CUDA not available. Please connect to a GPU instance if possible.\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi\n",
        "\n",
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  print('Using GPU')\n",
        "  device = 'cuda'\n",
        "else:\n",
        "  print('CUDA not available. Please connect to a GPU instance if possible.')\n",
        "  device = 'cpu'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/hkchengrex/Cutie.git\n",
        "%cd Cutie\n",
        "!pip install -e ."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YQtM2nKLA3CF",
        "outputId": "8423507d-95f9-461e-e592-ed30b03ff72d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Cutie'...\n",
            "remote: Enumerating objects: 609, done.\u001b[K\n",
            "remote: Counting objects: 100% (238/238), done.\u001b[K\n",
            "remote: Compressing objects: 100% (73/73), done.\u001b[K\n",
            "remote: Total 609 (delta 199), reused 165 (delta 165), pack-reused 371 (from 1)\u001b[K\n",
            "Receiving objects: 100% (609/609), 2.81 MiB | 10.69 MiB/s, done.\n",
            "Resolving deltas: 100% (308/308), done.\n",
            "/content/Cutie\n",
            "Obtaining file:///content/Cutie\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting thinplate@ git+https://github.com/cheind/py-thin-plate-spline (from cutie==1.0.0)\n",
            "  Cloning https://github.com/cheind/py-thin-plate-spline to /tmp/pip-install-epn_ji0t/thinplate_dc715d241d094420a98e2dd00eef5901\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/cheind/py-thin-plate-spline /tmp/pip-install-epn_ji0t/thinplate_dc715d241d094420a98e2dd00eef5901\n",
            "  Resolved https://github.com/cheind/py-thin-plate-spline to commit f6995795397118b7d0ac01aecd3f39ffbfad9dee\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting av>=0.5.2 (from cutie==1.0.0)\n",
            "  Downloading av-16.1.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (4.6 kB)\n",
            "Collecting cchardet>=2.1.7 (from cutie==1.0.0)\n",
            "  Downloading cchardet-2.1.7.tar.gz (653 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m653.6/653.6 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: charset-normalizer>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from cutie==1.0.0) (3.4.4)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.12/dist-packages (from cutie==1.0.0) (3.0.12)\n",
            "Requirement already satisfied: easydict in /usr/local/lib/python3.12/dist-packages (from cutie==1.0.0) (1.13)\n",
            "Requirement already satisfied: einops>=0.6 in /usr/local/lib/python3.12/dist-packages (from cutie==1.0.0) (0.8.1)\n",
            "Requirement already satisfied: gdown>=4.7.1 in /usr/local/lib/python3.12/dist-packages (from cutie==1.0.0) (5.2.0)\n",
            "Requirement already satisfied: gitpython>=3.1 in /usr/local/lib/python3.12/dist-packages (from cutie==1.0.0) (3.1.46)\n",
            "Requirement already satisfied: gradio>=3.34 in /usr/local/lib/python3.12/dist-packages (from cutie==1.0.0) (5.50.0)\n",
            "Collecting hickle>=5.0 (from cutie==1.0.0)\n",
            "  Downloading hickle-5.0.3-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting hydra-core>=1.3.2 (from cutie==1.0.0)\n",
            "  Downloading hydra_core-1.3.2-py3-none-any.whl.metadata (5.5 kB)\n",
            "Collecting netifaces>=0.11.0 (from cutie==1.0.0)\n",
            "  Downloading netifaces-0.11.0.tar.gz (30 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.21 in /usr/local/lib/python3.12/dist-packages (from cutie==1.0.0) (2.0.2)\n",
            "Requirement already satisfied: opencv-python>=4.8 in /usr/local/lib/python3.12/dist-packages (from cutie==1.0.0) (4.12.0.88)\n",
            "Requirement already satisfied: pillow>=9.5 in /usr/local/lib/python3.12/dist-packages (from cutie==1.0.0) (11.3.0)\n",
            "Requirement already satisfied: pycocotools>=2.0.7 in /usr/local/lib/python3.12/dist-packages (from cutie==1.0.0) (2.0.11)\n",
            "Collecting pyqtdarktheme (from cutie==1.0.0)\n",
            "  Downloading PyQtDarkTheme-0.1.7-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting pyside6>=6.2.0 (from cutie==1.0.0)\n",
            "  Downloading pyside6-6.10.1-cp39-abi3-manylinux_2_34_x86_64.whl.metadata (5.3 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from cutie==1.0.0) (2.32.4)\n",
            "Requirement already satisfied: scipy>=1.7 in /usr/local/lib/python3.12/dist-packages (from cutie==1.0.0) (1.16.3)\n",
            "Requirement already satisfied: tensorboard>=2.11 in /usr/local/lib/python3.12/dist-packages (from cutie==1.0.0) (2.19.0)\n",
            "Requirement already satisfied: tqdm>=4.66.1 in /usr/local/lib/python3.12/dist-packages (from cutie==1.0.0) (4.67.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (from gdown>=4.7.1->cutie==1.0.0) (4.13.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from gdown>=4.7.1->cutie==1.0.0) (3.20.2)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython>=3.1->cutie==1.0.0) (4.0.12)\n",
            "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.12/dist-packages (from gradio>=3.34->cutie==1.0.0) (24.1.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio>=3.34->cutie==1.0.0) (4.12.1)\n",
            "Requirement already satisfied: brotli>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from gradio>=3.34->cutie==1.0.0) (1.2.0)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.12/dist-packages (from gradio>=3.34->cutie==1.0.0) (0.123.10)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.12/dist-packages (from gradio>=3.34->cutie==1.0.0) (1.0.0)\n",
            "Requirement already satisfied: gradio-client==1.14.0 in /usr/local/lib/python3.12/dist-packages (from gradio>=3.34->cutie==1.0.0) (1.14.0)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.12/dist-packages (from gradio>=3.34->cutie==1.0.0) (0.1.2)\n",
            "Requirement already satisfied: httpx<1.0,>=0.24.1 in /usr/local/lib/python3.12/dist-packages (from gradio>=3.34->cutie==1.0.0) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub<2.0,>=0.33.5 in /usr/local/lib/python3.12/dist-packages (from gradio>=3.34->cutie==1.0.0) (0.36.0)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.12/dist-packages (from gradio>=3.34->cutie==1.0.0) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio>=3.34->cutie==1.0.0) (3.0.3)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio>=3.34->cutie==1.0.0) (3.11.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from gradio>=3.34->cutie==1.0.0) (25.0)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.12/dist-packages (from gradio>=3.34->cutie==1.0.0) (2.2.2)\n",
            "Requirement already satisfied: pydantic<=2.12.3,>=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio>=3.34->cutie==1.0.0) (2.12.3)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.12/dist-packages (from gradio>=3.34->cutie==1.0.0) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.12/dist-packages (from gradio>=3.34->cutie==1.0.0) (0.0.21)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.12/dist-packages (from gradio>=3.34->cutie==1.0.0) (6.0.3)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.12/dist-packages (from gradio>=3.34->cutie==1.0.0) (0.14.11)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.12/dist-packages (from gradio>=3.34->cutie==1.0.0) (0.1.7)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio>=3.34->cutie==1.0.0) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.12/dist-packages (from gradio>=3.34->cutie==1.0.0) (0.50.0)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from gradio>=3.34->cutie==1.0.0) (0.13.3)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.12/dist-packages (from gradio>=3.34->cutie==1.0.0) (0.21.1)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.12/dist-packages (from gradio>=3.34->cutie==1.0.0) (4.15.0)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.12/dist-packages (from gradio>=3.34->cutie==1.0.0) (0.40.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from gradio-client==1.14.0->gradio>=3.34->cutie==1.0.0) (2025.3.0)\n",
            "Requirement already satisfied: websockets<16.0,>=13.0 in /usr/local/lib/python3.12/dist-packages (from gradio-client==1.14.0->gradio>=3.34->cutie==1.0.0) (15.0.1)\n",
            "Requirement already satisfied: h5py>=2.10.0 in /usr/local/lib/python3.12/dist-packages (from hickle>=5.0->cutie==1.0.0) (3.15.1)\n",
            "Requirement already satisfied: omegaconf<2.4,>=2.2 in /usr/local/lib/python3.12/dist-packages (from hydra-core>=1.3.2->cutie==1.0.0) (2.3.0)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.12/dist-packages (from hydra-core>=1.3.2->cutie==1.0.0) (4.9.3)\n",
            "Collecting shiboken6==6.10.1 (from pyside6>=6.2.0->cutie==1.0.0)\n",
            "  Downloading shiboken6-6.10.1-cp39-abi3-manylinux_2_34_x86_64.whl.metadata (2.5 kB)\n",
            "Collecting PySide6_Essentials==6.10.1 (from pyside6>=6.2.0->cutie==1.0.0)\n",
            "  Downloading pyside6_essentials-6.10.1-cp39-abi3-manylinux_2_34_x86_64.whl.metadata (3.7 kB)\n",
            "Collecting PySide6_Addons==6.10.1 (from pyside6>=6.2.0->cutie==1.0.0)\n",
            "  Downloading pyside6_addons-6.10.1-cp39-abi3-manylinux_2_34_x86_64.whl.metadata (4.0 kB)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.12/dist-packages (from tensorboard>=2.11->cutie==1.0.0) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.12/dist-packages (from tensorboard>=2.11->cutie==1.0.0) (1.76.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard>=2.11->cutie==1.0.0) (3.10)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.12/dist-packages (from tensorboard>=2.11->cutie==1.0.0) (5.29.5)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard>=2.11->cutie==1.0.0) (75.2.0)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.12/dist-packages (from tensorboard>=2.11->cutie==1.0.0) (1.17.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard>=2.11->cutie==1.0.0) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard>=2.11->cutie==1.0.0) (3.1.5)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->cutie==1.0.0) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->cutie==1.0.0) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->cutie==1.0.0) (2026.1.4)\n",
            "Requirement already satisfied: torch>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from thinplate@ git+https://github.com/cheind/py-thin-plate-spline->cutie==1.0.0) (2.9.0+cpu)\n",
            "Requirement already satisfied: torchvision>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from thinplate@ git+https://github.com/cheind/py-thin-plate-spline->cutie==1.0.0) (0.24.0+cpu)\n",
            "Requirement already satisfied: annotated-doc>=0.0.2 in /usr/local/lib/python3.12/dist-packages (from fastapi<1.0,>=0.115.2->gradio>=3.34->cutie==1.0.0) (0.0.4)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython>=3.1->cutie==1.0.0) (5.0.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0,>=0.24.1->gradio>=3.34->cutie==1.0.0) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0,>=0.24.1->gradio>=3.34->cutie==1.0.0) (0.16.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.33.5->gradio>=3.34->cutie==1.0.0) (1.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0,>=1.0->gradio>=3.34->cutie==1.0.0) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0,>=1.0->gradio>=3.34->cutie==1.0.0) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0,>=1.0->gradio>=3.34->cutie==1.0.0) (2025.3)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<=2.12.3,>=2.0->gradio>=3.34->cutie==1.0.0) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<=2.12.3,>=2.0->gradio>=3.34->cutie==1.0.0) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<=2.12.3,>=2.0->gradio>=3.34->cutie==1.0.0) (0.4.2)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=0.4.0->thinplate@ git+https://github.com/cheind/py-thin-plate-spline->cutie==1.0.0) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=0.4.0->thinplate@ git+https://github.com/cheind/py-thin-plate-spline->cutie==1.0.0) (3.6.1)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio>=3.34->cutie==1.0.0) (8.3.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio>=3.34->cutie==1.0.0) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio>=3.34->cutie==1.0.0) (13.9.4)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->gdown>=4.7.1->cutie==1.0.0) (2.8.1)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown>=4.7.1->cutie==1.0.0) (1.7.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio>=3.34->cutie==1.0.0) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio>=3.34->cutie==1.0.0) (2.19.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=0.4.0->thinplate@ git+https://github.com/cheind/py-thin-plate-spline->cutie==1.0.0) (1.3.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio>=3.34->cutie==1.0.0) (0.1.2)\n",
            "Downloading av-16.1.0-cp312-cp312-manylinux_2_28_x86_64.whl (41.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.2/41.2 MB\u001b[0m \u001b[31m28.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading hickle-5.0.3-py3-none-any.whl (107 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108.0/108.0 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading hydra_core-1.3.2-py3-none-any.whl (154 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.5/154.5 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyside6-6.10.1-cp39-abi3-manylinux_2_34_x86_64.whl (557 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.8/557.8 kB\u001b[0m \u001b[31m38.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyside6_addons-6.10.1-cp39-abi3-manylinux_2_34_x86_64.whl (170.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m170.7/170.7 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyside6_essentials-6.10.1-cp39-abi3-manylinux_2_34_x86_64.whl (76.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.8/76.8 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading shiboken6-6.10.1-cp39-abi3-manylinux_2_34_x86_64.whl (271 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m272.0/272.0 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading PyQtDarkTheme-0.1.7-py3-none-any.whl (99 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.6/99.6 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: cutie, cchardet, netifaces, thinplate\n",
            "  Building editable for cutie (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for cutie: filename=cutie-1.0.0-py3-none-any.whl size=4557 sha256=b2ff65de2832e544a88c7d2ce97243de1eefac2b88fbcba8ccaad1a25665908d\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-vvkifcpf/wheels/54/98/40/fc7af58c732d3bdcd8fcd7a553a972f7cc75b4be607742991e\n",
            "  Building wheel for cchardet (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for cchardet: filename=cchardet-2.1.7-cp312-cp312-linux_x86_64.whl size=300350 sha256=24d83f269727ade24ef2645f72203663887097f4bfac9e86f73a8a349de8a544\n",
            "  Stored in directory: /root/.cache/pip/wheels/e4/40/3c/290a62e1fa01b0c46e93720c12eb1e97028f81ff04368b6b56\n",
            "  Building wheel for netifaces (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for netifaces: filename=netifaces-0.11.0-cp312-cp312-linux_x86_64.whl size=36188 sha256=a9cbc65701a33cdc6d4f77b718553dae7ffdcc94911643631030a08d68d8a0bf\n",
            "  Stored in directory: /root/.cache/pip/wheels/63/fa/57/da80d0ffc8f993315c479b7cd4c8fb1c23910c8baccf6b1b27\n",
            "  Building wheel for thinplate (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for thinplate: filename=thinplate-1.0.0-py3-none-any.whl size=6701 sha256=d929f852e9955f45afc00c8b838b24a41e717576b09f4856d27c178b86bac182\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-vvkifcpf/wheels/65/b2/ff/baba3d814b978be1f20df5b7a642a341d0e8ebafd1ea260718\n",
            "Successfully built cutie cchardet netifaces thinplate\n",
            "Installing collected packages: netifaces, cchardet, shiboken6, pyqtdarktheme, av, PySide6_Essentials, hydra-core, hickle, PySide6_Addons, thinplate, pyside6, cutie\n",
            "Successfully installed PySide6_Addons-6.10.1 PySide6_Essentials-6.10.1 av-16.1.0 cchardet-2.1.7 cutie-1.0.0 hickle-5.0.3 hydra-core-1.3.2 netifaces-0.11.0 pyqtdarktheme-0.1.7 pyside6-6.10.1 shiboken6-6.10.1 thinplate-1.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/Cutie\n",
        "!python cutie/utils/download_models.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_qIRplQjA42u",
        "outputId": "3d115bea-1f58-4263-897a-76c284d679d8"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Cutie\n",
            "Downloading coco_lvis_h18_itermask.pth to /content/Cutie/cutie/utils/../../weights...\n",
            "100% 40.7M/40.7M [00:01<00:00, 26.8MiB/s]\n",
            "Downloading cutie-base-mega.pth to /content/Cutie/cutie/utils/../../weights...\n",
            "100% 140M/140M [00:05<00:00, 26.6MiB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/Cutie/\n",
        "\n",
        "import torch\n",
        "from omegaconf import open_dict\n",
        "from hydra import compose, initialize_config_dir\n",
        "from hydra.core.global_hydra import GlobalHydra\n",
        "\n",
        "from cutie.model.cutie import CUTIE\n",
        "from cutie.inference.utils.args_utils import get_dataset_cfg\n",
        "\n",
        "# ---- choose device ----\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "# 如果你想强制不用GPU，就取消下一行注释：\n",
        "# DEVICE = \"cpu\"\n",
        "\n",
        "# ---- clear hydra (notebook) ----\n",
        "if GlobalHydra.instance().is_initialized():\n",
        "    GlobalHydra.instance().clear()\n",
        "\n",
        "with torch.inference_mode():\n",
        "    initialize_config_dir(\n",
        "        version_base=\"1.3.2\",\n",
        "        config_dir=\"/content/Cutie/cutie/config\",\n",
        "        job_name=\"eval_config\",\n",
        "    )\n",
        "    cfg = compose(config_name=\"eval_config\")\n",
        "\n",
        "    with open_dict(cfg):\n",
        "        cfg[\"weights\"] = \"./weights/cutie-base-mega.pth\"\n",
        "        if cfg.get(\"mem_every\", None) is None:\n",
        "            cfg[\"mem_every\"] = 5\n",
        "        if cfg.get(\"stagger_updates\", None) is None:\n",
        "            cfg[\"stagger_updates\"] = 0\n",
        "\n",
        "    _ = get_dataset_cfg(cfg)\n",
        "\n",
        "    cutie = CUTIE(cfg).to(DEVICE).eval()\n",
        "    model_weights = torch.load(cfg.weights, map_location=DEVICE)\n",
        "    cutie.load_weights(model_weights)\n",
        "\n",
        "print(\"CUTIE loaded OK:\", cfg.weights, \"| device:\", DEVICE)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CgKoWLWxA79e",
        "outputId": "804d1d51-800e-43b4-a778-6369f088ed01"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Cutie\n",
            "CUTIE loaded OK: ./weights/cutie-base-mega.pth | device: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, cv2, tempfile\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import torch\n",
        "import gradio as gr\n",
        "from omegaconf import open_dict\n",
        "\n",
        "import traceback\n",
        "from cutie.inference.inference_core import InferenceCore\n",
        "from gui.interactive_utils import image_to_torch, torch_prob_to_numpy_mask, index_numpy_to_one_hot_torch, overlay_davis\n",
        "\n",
        "DEFAULT_VIDEO = \"echo[1].mp4\"\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "def _resolve_video_path(p):\n",
        "    cand = [p, os.path.join(\"/content\", p)]\n",
        "    for c in cand:\n",
        "        if os.path.exists(c):\n",
        "            return c\n",
        "    raise gr.Error(f\"Video not found: {p}\")\n",
        "\n",
        "def _get_video_info(video_path):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    if not cap.isOpened():\n",
        "        raise gr.Error(f\"Cannot open video: {video_path}\")\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "    fps = 30.0 if (fps is None or fps <= 1e-3) else float(fps)\n",
        "    n = int(cap.get(cv2.CAP_PROP_FRAME_COUNT) or 0)\n",
        "    w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH) or 0)\n",
        "    h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT) or 0)\n",
        "    cap.release()\n",
        "    return fps, n, w, h\n",
        "\n",
        "def _read_frame(video_path, frame_idx):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    if not cap.isOpened():\n",
        "        raise gr.Error(f\"Cannot open video: {video_path}\")\n",
        "    cap.set(cv2.CAP_PROP_POS_FRAMES, int(frame_idx))\n",
        "    ok, frame = cap.read()\n",
        "    cap.release()\n",
        "    if (not ok) or frame is None:\n",
        "        raise gr.Error(f\"Failed to read frame {frame_idx}\")\n",
        "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "    return frame, Image.fromarray(frame_rgb)\n",
        "\n",
        "def _editor_value_from_frame(frame_pil):\n",
        "    # 关键：让你“在这一帧上画”，不是黑底\n",
        "    # ImageEditor 需要 composite 字段，否则你之前会 KeyError\n",
        "    return {\"background\": frame_pil, \"layers\": [], \"composite\": frame_pil}\n",
        "\n",
        "def _mask_from_editor(editor_value):\n",
        "    \"\"\"\n",
        "    从 ImageEditor 取 mask：用 composite 和 background 的像素差分得到前景区域\n",
        "    你在帧上画的地方会改变 composite 像素 -> diff>阈值 -> mask=1\n",
        "    \"\"\"\n",
        "    if editor_value is None:\n",
        "        raise gr.Error(\"Mask editor is empty. Please paint on the frame.\")\n",
        "    bg = editor_value.get(\"background\", None)\n",
        "    comp = editor_value.get(\"composite\", None) or bg\n",
        "    if bg is None or comp is None:\n",
        "        raise gr.Error(\"ImageEditor returned no background/composite.\")\n",
        "\n",
        "    bg = bg.convert(\"RGB\")\n",
        "    comp = comp.convert(\"RGB\")\n",
        "    bg_arr = np.array(bg).astype(np.int16)\n",
        "    cp_arr = np.array(comp).astype(np.int16)\n",
        "\n",
        "    if bg_arr.shape != cp_arr.shape:\n",
        "        raise gr.Error(\"Editor output size mismatch. Try reloading the frame.\")\n",
        "\n",
        "    diff = np.abs(cp_arr - bg_arr).sum(axis=-1)  # H,W\n",
        "    mask = (diff > 25).astype(np.uint8)          # 阈值可调：越大越不敏感\n",
        "    return mask\n",
        "\n",
        "def _save_overlay_video(frames_bgr, fps, out_mp4):\n",
        "    h, w = frames_bgr[0].shape[:2]\n",
        "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
        "    vw = cv2.VideoWriter(out_mp4, fourcc, fps, (w, h))\n",
        "    for f in frames_bgr:\n",
        "        vw.write(f)\n",
        "    vw.release()\n",
        "\n",
        "\n",
        "def load_video(video_path_str):\n",
        "    vp = _resolve_video_path(video_path_str)\n",
        "    fps, n, w, h = _get_video_info(vp)\n",
        "    _, frame0_pil = _read_frame(vp, 0)\n",
        "    editor_init = _editor_value_from_frame(frame0_pil)\n",
        "\n",
        "    info = f\"Loaded: {vp} | frames={n} | fps={fps:.2f} | size={w}x{h}\"\n",
        "\n",
        "    # 选帧 slider：0..n-1\n",
        "    frame_idx_update = gr.update(minimum=0, maximum=max(0, n-1), value=0, step=1)\n",
        "\n",
        "    # frames_to_propagate 上限：n-1-当前帧（当前帧=0）\n",
        "    max_prop = max(1, (n - 1) - 0)\n",
        "    frames_to_prop_update = gr.update(minimum=1, maximum=max_prop, value=min(200, max_prop), step=1)\n",
        "\n",
        "    # max_internal_size：建议不超过 max(w,h)，越小越快越糊\n",
        "    max_side = max(w, h)\n",
        "    default_mis = min(max_side, 720)  # 你也可以改成 640/800 等\n",
        "    max_internal_update = gr.update(minimum=256, maximum=max(256, max_side), value=default_mis, step=32)\n",
        "\n",
        "    return vp, frame0_pil, editor_init, frame_idx_update, frames_to_prop_update, max_internal_update, info\n",
        "\n",
        "\n",
        "def show_frame(video_path_str, frame_idx):\n",
        "    vp = _resolve_video_path(video_path_str)\n",
        "    fps, n, w, h = _get_video_info(vp)\n",
        "\n",
        "    frame_idx = int(frame_idx)\n",
        "    _, frame_pil = _read_frame(vp, frame_idx)\n",
        "\n",
        "    # 切到该帧时，frames_to_propagate 上限跟着变：n-1-当前帧\n",
        "    max_prop = max(1, (n - 1) - frame_idx)\n",
        "    frames_to_prop_update = gr.update(minimum=1, maximum=max_prop, value=min(200, max_prop), step=1)\n",
        "\n",
        "    return frame_pil, _editor_value_from_frame(frame_pil), frames_to_prop_update\n",
        "\n",
        "def run_track(video_path_str, start_frame_idx, editor_value, frames_to_propagate, max_internal_size):\n",
        "    vp = _resolve_video_path(video_path_str)\n",
        "    fps, n, w, h = _get_video_info(vp)\n",
        "\n",
        "    start_frame_idx = int(start_frame_idx)\n",
        "    frames_to_propagate = int(frames_to_propagate)\n",
        "    max_internal_size = int(max_internal_size)\n",
        "\n",
        "    remaining = (n - 1) - start_frame_idx\n",
        "    if remaining <= 0:\n",
        "        raise gr.Error(f\"No remaining frames from start_frame={start_frame_idx}. video_frames={n}\")\n",
        "    frames_to_propagate = max(1, min(frames_to_propagate, remaining))\n",
        "\n",
        "    mask_index = _mask_from_editor(editor_value)\n",
        "    if mask_index.sum() < 10:\n",
        "        raise gr.Error(\"Mask too small / empty. Please paint a larger region on the frame.\")\n",
        "\n",
        "    # 关键：mask resize 回视频尺寸\n",
        "    if mask_index.shape[0] != h or mask_index.shape[1] != w:\n",
        "        mask_index = cv2.resize(mask_index, (w, h), interpolation=cv2.INTER_NEAREST)\n",
        "\n",
        "    # 写入 cfg\n",
        "    with open_dict(cfg):\n",
        "        cfg[\"max_internal_size\"] = max_internal_size\n",
        "\n",
        "    processor = InferenceCore(cutie, cfg=cfg)\n",
        "\n",
        "    cap = cv2.VideoCapture(vp)\n",
        "    if not cap.isOpened():\n",
        "        raise gr.Error(f\"Cannot open video: {vp}\")\n",
        "    cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame_idx)\n",
        "\n",
        "    overlay_frames_bgr = []\n",
        "    current = 0\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    with torch.inference_mode():\n",
        "        #with torch.amp.autocast(device_type=\"cuda\", enabled=(DEVICE == \"cuda\")):\n",
        "\n",
        "        amp_device = \"cuda\" if DEVICE == \"cuda\" else \"cpu\"\n",
        "        with torch.amp.autocast(device_type=amp_device, enabled=(DEVICE == \"cuda\")):\n",
        "\n",
        "\n",
        "            while cap.isOpened() and current < frames_to_propagate:\n",
        "                ok, frame = cap.read()\n",
        "                if (not ok) or frame is None:\n",
        "                    break\n",
        "\n",
        "                frame_torch = image_to_torch(frame, device=DEVICE)\n",
        "\n",
        "                if current == 0:\n",
        "                    # 打印确认 max_internal_size 已写进去\n",
        "                    print(\"cfg.max_internal_size =\", cfg.get(\"max_internal_size\", None))\n",
        "\n",
        "                    mask_torch = index_numpy_to_one_hot_torch(mask_index, 2).to(DEVICE)  # num_objects=1 => 2\n",
        "                    pred = processor.step(frame_torch, mask_torch[1:], idx_mask=False)\n",
        "                else:\n",
        "                    pred = processor.step(frame_torch)\n",
        "\n",
        "                pred_index = torch_prob_to_numpy_mask(pred)  # 0/1\n",
        "\n",
        "                vis = overlay_davis(frame, pred_index)\n",
        "\n",
        "                # overlay_davis 有时返回 PIL / np / RGB/BGR 不一致：这里统一成 BGR 写视频\n",
        "                if isinstance(vis, Image.Image):\n",
        "                    vis = np.array(vis)\n",
        "                if vis.dtype != np.uint8:\n",
        "                    vis = vis.astype(np.uint8)\n",
        "\n",
        "                # 如果 vis 看起来是 RGB（多数情况），转 BGR；如果它已经像 BGR（跟原frame更接近），不转\n",
        "                if vis.ndim == 3 and vis.shape[2] == 3:\n",
        "                    diff_as_bgr = np.mean(np.abs(vis.astype(np.int16) - frame.astype(np.int16)))\n",
        "                    vis_as_bgr_from_rgb = cv2.cvtColor(vis, cv2.COLOR_RGB2BGR)\n",
        "                    diff_as_rgb = np.mean(np.abs(vis_as_bgr_from_rgb.astype(np.int16) - frame.astype(np.int16)))\n",
        "                    vis_bgr = vis if diff_as_bgr <= diff_as_rgb else vis_as_bgr_from_rgb\n",
        "                else:\n",
        "                    vis_bgr = frame  # 异常情况就退回原帧\n",
        "\n",
        "                overlay_frames_bgr.append(vis_bgr)\n",
        "                current += 1\n",
        "\n",
        "    cap.release()\n",
        "\n",
        "    if len(overlay_frames_bgr) == 0:\n",
        "        raise gr.Error(\"No frames processed. Check video path / start_frame.\")\n",
        "\n",
        "    out_dir = tempfile.mkdtemp(prefix=\"cutie_ui_\")\n",
        "    overlay_mp4 = os.path.join(out_dir, \"overlay.mp4\")\n",
        "    _save_overlay_video(overlay_frames_bgr, fps, overlay_mp4)\n",
        "\n",
        "    status = (\n",
        "        f\"Done. video={os.path.basename(vp)} | size={w}x{h} | fps={fps:.2f} | \"\n",
        "        f\"start={start_frame_idx} | processed={len(overlay_frames_bgr)} | \"\n",
        "        f\"frames_to_propagate(clamped)={frames_to_propagate} | max_internal_size={max_internal_size}\"\n",
        "    )\n",
        "    return overlay_mp4, status\n",
        "\n",
        "\n",
        "def run_track_safe(video_path_str, start_frame_idx, editor_value, frames_to_propagate, max_internal_size):\n",
        "    try:\n",
        "        return run_track(video_path_str, start_frame_idx, editor_value, frames_to_propagate, max_internal_size)\n",
        "    except Exception as e:\n",
        "        traceback.print_exc()\n",
        "        raise gr.Error(str(e))\n",
        "\n",
        "\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"## CUTIE (Preview video in Gradio + draw mask on a selected frame)\")\n",
        "\n",
        "    video_path = gr.Textbox(label=\"Video path (in /content)\", value=DEFAULT_VIDEO)\n",
        "    with gr.Row():\n",
        "        load_btn = gr.Button(\"Load video\")\n",
        "        info = gr.Textbox(label=\"Info\", interactive=False)\n",
        "\n",
        "    with gr.Row():\n",
        "        orig_video = gr.Video(label=\"Original Video (preview here)\")\n",
        "        overlay_video = gr.Video(label=\"Overlay Video (result preview)\")\n",
        "\n",
        "    gr.Markdown(\"### 1) Use the video player to preview (pause/seek).  2) Choose a frame index below to annotate (Gradio can't read the paused timestamp).\")\n",
        "\n",
        "    with gr.Row():\n",
        "        frame_idx = gr.Slider(0, 0, value=0, step=1, label=\"Frame index to annotate (acts like pause point)\")\n",
        "        show_btn = gr.Button(\"Load this frame for annotation\")\n",
        "\n",
        "    with gr.Row():\n",
        "        frame_view = gr.Image(label=\"Selected Frame\", type=\"pil\", interactive=False)\n",
        "        mask_editor = gr.ImageEditor(label=\"Paint directly ON the frame (your strokes define the mask)\", type=\"pil\")\n",
        "\n",
        "    #frames_to_prop = gr.Slider(1, 1000, value=200, step=1, label=\"frames_to_propagate\")\n",
        "\n",
        "\n",
        "    #frames_to_prop = gr.Slider(1, 1, value=1, step=1, label=\"frames_to_propagate (auto limited)\")\n",
        "\n",
        "    frames_to_prop = gr.Slider(1, 1, value=200, step=1, label=\"frames_to_propagate (auto max = remaining frames)\")\n",
        "\n",
        "    max_internal_size = gr.Slider(\n",
        "    256, 1024, value=720, step=32,\n",
        "    label=\"max_internal_size (max internal side; smaller=faster, lower quality)\"\n",
        ")\n",
        "\n",
        "    run_btn = gr.Button(\"Run CUTIE from this frame\")\n",
        "    status = gr.Textbox(label=\"Status\", interactive=False)\n",
        "\n",
        "\n",
        "    load_btn.click(\n",
        "    load_video,\n",
        "    inputs=[video_path],\n",
        "    outputs=[orig_video, frame_view, mask_editor, frame_idx, frames_to_prop, max_internal_size, info],\n",
        "    queue=False\n",
        ")\n",
        "\n",
        "    show_btn.click(\n",
        "    show_frame,\n",
        "    inputs=[video_path, frame_idx],\n",
        "    outputs=[frame_view, mask_editor, frames_to_prop],\n",
        "    queue=False\n",
        ")\n",
        "    run_btn.click(\n",
        "    run_track_safe,\n",
        "    inputs=[video_path, frame_idx, mask_editor, frames_to_prop, max_internal_size],\n",
        "    outputs=[overlay_video, status],\n",
        "    queue=True   # 建议 True，更稳，避免 “Unexpected token '<'”\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "demo.launch(debug=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "osSkA29oA8vw",
        "outputId": "495cdef7-0457-4b8f-84cc-4ec62e664798"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/Cutie/cutie/model/big_modules.py:289: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=False):\n",
            "/content/Cutie/cutie/model/modules.py:62: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=False):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://eae439bf074393f571.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/Cutie/cutie/model/modules.py:62: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=False):\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://eae439bf074393f571.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/Cutie/cutie/utils/tensor_utils.py:48: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=False):\n",
            "/content/Cutie/cutie/utils/tensor_utils.py:48: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=False):\n",
            "/content/Cutie/cutie/model/big_modules.py:289: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=False):\n",
            "/content/Cutie/cutie/model/modules.py:62: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=False):\n",
            "/content/Cutie/cutie/model/modules.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=False):\n",
            "/content/Cutie/cutie/model/transformer/object_summarizer.py:78: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=False):\n",
            "/content/Cutie/cutie/utils/tensor_utils.py:48: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=False):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cfg.max_internal_size = 720\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/Cutie/cutie/model/big_modules.py:289: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=False):\n",
            "/content/Cutie/cutie/model/modules.py:62: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=False):\n",
            "/content/Cutie/cutie/model/modules.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=False):\n",
            "/content/Cutie/cutie/model/transformer/object_summarizer.py:78: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=False):\n",
            "/usr/local/lib/python3.12/dist-packages/gradio/components/video.py:398: UserWarning: Video does not have browser-compatible container or codec. Converting to mp4.\n",
            "  warnings.warn(\n",
            "/content/Cutie/cutie/utils/tensor_utils.py:48: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=False):\n",
            "/content/Cutie/cutie/model/big_modules.py:289: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=False):\n",
            "/content/Cutie/cutie/model/modules.py:62: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=False):\n",
            "/content/Cutie/cutie/model/big_modules.py:289: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=False):\n",
            "/content/Cutie/cutie/model/modules.py:62: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=False):\n",
            "/content/Cutie/cutie/utils/tensor_utils.py:48: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=False):\n",
            "/content/Cutie/cutie/model/modules.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=False):\n",
            "/content/Cutie/cutie/model/transformer/object_summarizer.py:78: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=False):\n",
            "/content/Cutie/cutie/utils/tensor_utils.py:48: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=False):\n",
            "/content/Cutie/cutie/model/big_modules.py:289: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=False):\n",
            "/content/Cutie/cutie/model/modules.py:62: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=False):\n",
            "/content/Cutie/cutie/utils/tensor_utils.py:48: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=False):\n",
            "/content/Cutie/cutie/utils/tensor_utils.py:48: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=False):\n",
            "/content/Cutie/cutie/model/big_modules.py:289: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=False):\n",
            "/content/Cutie/cutie/model/modules.py:62: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=False):\n",
            "/content/Cutie/cutie/utils/tensor_utils.py:48: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=False):\n",
            "/content/Cutie/cutie/utils/tensor_utils.py:48: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=False):\n",
            "/content/Cutie/cutie/utils/tensor_utils.py:48: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=False):\n",
            "/content/Cutie/cutie/utils/tensor_utils.py:48: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=False):\n",
            "/content/Cutie/cutie/model/big_modules.py:289: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=False):\n",
            "/content/Cutie/cutie/model/modules.py:62: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=False):\n",
            "/content/Cutie/cutie/utils/tensor_utils.py:48: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=False):\n",
            "/content/Cutie/cutie/model/big_modules.py:289: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=False):\n",
            "/content/Cutie/cutie/model/modules.py:62: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=False):\n",
            "/content/Cutie/cutie/utils/tensor_utils.py:48: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=False):\n",
            "/content/Cutie/cutie/utils/tensor_utils.py:48: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=False):\n",
            "/content/Cutie/cutie/model/big_modules.py:289: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=False):\n",
            "/content/Cutie/cutie/model/modules.py:62: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=False):\n",
            "/content/Cutie/cutie/utils/tensor_utils.py:48: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=False):\n",
            "/content/Cutie/cutie/model/big_modules.py:289: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=False):\n",
            "/content/Cutie/cutie/model/modules.py:62: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=False):\n",
            "/content/Cutie/cutie/utils/tensor_utils.py:48: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=False):\n",
            "/content/Cutie/cutie/utils/tensor_utils.py:48: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=False):\n",
            "/content/Cutie/cutie/utils/tensor_utils.py:48: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=False):\n",
            "/content/Cutie/cutie/model/big_modules.py:289: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=False):\n",
            "/content/Cutie/cutie/model/modules.py:62: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=False):\n",
            "/content/Cutie/cutie/utils/tensor_utils.py:48: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=False):\n",
            "/content/Cutie/cutie/utils/tensor_utils.py:48: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=False):\n",
            "/content/Cutie/cutie/model/big_modules.py:289: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=False):\n",
            "/content/Cutie/cutie/model/modules.py:62: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=False):\n",
            "/content/Cutie/cutie/model/modules.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=False):\n",
            "/content/Cutie/cutie/utils/tensor_utils.py:48: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=False):\n",
            "/content/Cutie/cutie/model/transformer/object_summarizer.py:78: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=False):\n",
            "/content/Cutie/cutie/model/modules.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=False):\n",
            "/content/Cutie/cutie/model/transformer/object_summarizer.py:78: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=False):\n",
            "/content/Cutie/cutie/utils/tensor_utils.py:48: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=False):\n",
            "/content/Cutie/cutie/model/big_modules.py:289: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=False):\n",
            "/content/Cutie/cutie/model/modules.py:62: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=False):\n",
            "/content/Cutie/cutie/utils/tensor_utils.py:48: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=False):\n",
            "/content/Cutie/cutie/model/big_modules.py:289: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=False):\n",
            "/content/Cutie/cutie/model/modules.py:62: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=False):\n",
            "/content/Cutie/cutie/utils/tensor_utils.py:48: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=False):\n",
            "/content/Cutie/cutie/utils/tensor_utils.py:48: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=False):\n",
            "/content/Cutie/cutie/model/big_modules.py:289: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=False):\n",
            "/content/Cutie/cutie/model/modules.py:62: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=False):\n",
            "/content/Cutie/cutie/utils/tensor_utils.py:48: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=False):\n",
            "/content/Cutie/cutie/utils/tensor_utils.py:48: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=False):\n",
            "/content/Cutie/cutie/model/big_modules.py:289: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=False):\n",
            "/content/Cutie/cutie/model/modules.py:62: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=False):\n",
            "/content/Cutie/cutie/utils/tensor_utils.py:48: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=False):\n",
            "/content/Cutie/cutie/utils/tensor_utils.py:48: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=False):\n",
            "/content/Cutie/cutie/utils/tensor_utils.py:48: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=False):\n",
            "/content/Cutie/cutie/utils/tensor_utils.py:48: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=False):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://eae439bf074393f571.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!grep -R \"max_internal_size\" -n /content/Cutie/cutie | head\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8_fhZ9bYyi32",
        "outputId": "a93aeb72-ec41-4047-abcc-4b03edb2c930"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "grep: /content/Cutie/cutie/inference/__pycache__/inference_core.cpython-312.pyc: binary file matches\n",
            "/content/Cutie/cutie/config/video_config.yaml:20:max_internal_size: 480\n",
            "/content/Cutie/cutie/config/gui_config.yaml:17:max_internal_size: 480\n",
            "/content/Cutie/cutie/config/eval_config.yaml:23:max_internal_size: -1\n",
            "/content/Cutie/cutie/inference/inference_core.py:31:        self.max_internal_size = cfg.max_internal_size\n",
            "/content/Cutie/cutie/inference/inference_core.py:208:        if self.max_internal_size > 0:\n",
            "/content/Cutie/cutie/inference/inference_core.py:211:            if min_side > self.max_internal_size:\n",
            "/content/Cutie/cutie/inference/inference_core.py:213:                new_h = int(h / min_side * self.max_internal_size)\n",
            "/content/Cutie/cutie/inference/inference_core.py:214:                new_w = int(w / min_side * self.max_internal_size)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7zCVtDJcA-b1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}